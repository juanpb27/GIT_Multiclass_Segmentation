{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GIT_Multiclass_Segmentation\n-------------------\nhttps://github.com/juanpb27/GIT_Multiclass_Segmentation","metadata":{}},{"cell_type":"markdown","source":"# Helpful libraries and functions\n-----------","metadata":{}},{"cell_type":"code","source":"# System operations\nimport re\nimport os\nimport glob\nimport shutil\nimport splitfolders #!pip install split-folders\nimport patoolib #!pip install patool\n\n# Handling data\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Computer Vision and plotting\nimport cv2\nfrom PIL import Image\nimport seaborn as sns\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n# Machine and Deep Learning\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.models import Model\nfrom sklearn.metrics import fbeta_score\nfrom tensorflow.keras.metrics import MeanIoU\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Conv2DTranspose, BatchNormalization, Dropout, Lambda\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data paths\n-----------","metadata":{}},{"cell_type":"code","source":"# Challenge images and annotations\nPROJECT_PATH = '../'\nINPUT_PATH = PROJECT_PATH + 'input/'\nCHALLENGE_PATH = INPUT_PATH + 'uw-madison-gi-tract-image-segmentation/'\nIMAGES = CHALLENGE_PATH + 'train'\nLABELS = CHALLENGE_PATH + 'train.csv'\n\n# Built dataset to train a CNN\nTRAIN_IMAGES_PATH = INPUT_PATH + 'uwmgi-dataset-splitted/train/images/'\nTRAIN_MASKS_PATH = INPUT_PATH + 'uwmgi-dataset-splitted/train/masks/'\nVALID_IMAGES_PATH = INPUT_PATH + 'uwmgi-dataset-splitted/valid/images/'\nVALID_MASKS_PATH = INPUT_PATH + 'uwmgi-dataset-splitted/valid/masks/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Another used Functions\n------------","metadata":{}},{"cell_type":"code","source":"# Obtaining images metadata\ndef get_metadata(path):\n    \n    metadata = re.search('\\d{3}_\\d{3}_\\d{1}.\\d{2}_\\d{1}.\\d{2}', path)\n    \n    width    = metadata.group()[0:3]\n    height   = metadata.group()[4:7]\n    m_width  = metadata.group()[8:12]\n    m_height = metadata.group()[13:]\n    \n    return height, width, m_height, m_width","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding RLE to masks\n# Ref: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle2mask(mask_rle, label, shape):\n    \"\"\"\n    mask_rle: run-length as string formatted (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = label\n    return img.reshape(shape)  # Needed to align to RLE direction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data to be used in the CNN\ndef PrepareData(X, Y, n_classes):\n    \n    X = X / (2 ** 16 - 1.) # Normalization is performed by 2^16 because the image is uint16\n    Y = to_categorical(Y, n_classes) # To split the mask in 4 binary channels for the classes\n    \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:41:15.877449Z","iopub.execute_input":"2022-06-17T00:41:15.878227Z","iopub.status.idle":"2022-06-17T00:41:15.88493Z","shell.execute_reply.started":"2022-06-17T00:41:15.878187Z","shell.execute_reply":"2022-06-17T00:41:15.884087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a tensor to feed the DataGenerator and the CNN\ndef CreateSet(images_path, masks_path, n_classes, seed_sample):\n    \n    X = []  \n    Y = [] \n\n    sample = len(os.listdir(images_path)) // 10 # Sample of 10%\n    #sample = len(os.listdir(images_path)) // 5 # Sample of 20%\n    #sample = len(os.listdir(images_path)) // 3 # Sample of 33.3%\n    \n    list_images = os.listdir(images_path)\n    list_images.sort()\n    list_images = list_images[seed_sample : seed_sample + sample]\n\n    list_masks = os.listdir(masks_path)\n    list_masks.sort()\n    list_masks = list_masks[seed_sample : seed_sample + sample]\n\n\n    for i, image_name in enumerate(list_images):\n        image = cv2.imread(images_path + image_name, -1)\n        #image = image.astype('uint8')\n        image = Image.fromarray(image)\n\n        X.append(np.array(image))\n\n    for i, mask_name in enumerate(list_masks):\n        mask = cv2.imread(masks_path + mask_name, -1)\n        mask = Image.fromarray(mask)\n\n        Y.append(np.array(mask))\n        \n    X = np.array(X)\n    X = X[...,None] # Expand dims\n    \n    Y = np.array(Y)\n    Y = Y[...,None]\n        \n    #X, Y = PrepareData(X, Y, n_classes)\n        \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:41:15.864871Z","iopub.execute_input":"2022-06-17T00:41:15.865461Z","iopub.status.idle":"2022-06-17T00:41:15.875836Z","shell.execute_reply.started":"2022-06-17T00:41:15.865431Z","shell.execute_reply":"2022-06-17T00:41:15.874896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looped Generation of a batch of differents images\ndef Generator(X, Y, n_classes):\n    \n    img_data_gen_args = dict(horizontal_flip=True,\n                      vertical_flip=True,\n                      fill_mode='reflect')\n    \n    image_datagen = ImageDataGenerator(**img_data_gen_args)\n    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n    \n    image_generator = image_datagen.flow(\n        X,\n        batch_size = batch_size,\n        seed = seed)\n    \n    mask_generator = mask_datagen.flow(\n        Y,\n        batch_size = batch_size,\n        seed = seed)\n    \n    generator = zip(image_generator, mask_generator)\n    \n    for (img, mask) in generator:\n        img, mask = PrepareData(img, mask, n_classes)\n        yield (img, mask)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:44:56.632559Z","iopub.execute_input":"2022-06-17T00:44:56.633329Z","iopub.status.idle":"2022-06-17T00:44:56.643241Z","shell.execute_reply.started":"2022-06-17T00:44:56.633284Z","shell.execute_reply":"2022-06-17T00:44:56.642185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Used architecture(U-Net)\n\n# https://github.com/bnsreenu/python_for_microscopists/blob/master/208-simple_multi_unet_model.py\n\"\"\"\nStandard Unet\nModel not compiled here, instead will be done externally to make it\neasy to test various loss functions and optimizers. \n\"\"\"\n################################################################\ndef multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n#Build the model\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n    s = inputs\n\n    #Contraction path\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = Dropout(0.1)(c1)\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n    \n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = Dropout(0.1)(c2)\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n     \n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n     \n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n     \n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n    \n    #Expansive path \n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n     \n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n     \n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = Dropout(0.1)(c8)\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n     \n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = Dropout(0.1)(c9)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n     \n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n     \n    model = Model(inputs=[inputs], outputs=[outputs])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:02.565006Z","iopub.execute_input":"2022-06-17T00:45:02.565692Z","iopub.status.idle":"2022-06-17T00:45:02.590705Z","shell.execute_reply.started":"2022-06-17T00:45:02.565653Z","shell.execute_reply":"2022-06-17T00:45:02.589771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:13.914118Z","iopub.execute_input":"2022-06-17T00:45:13.91521Z","iopub.status.idle":"2022-06-17T00:45:13.920594Z","shell.execute_reply.started":"2022-06-17T00:45:13.915164Z","shell.execute_reply":"2022-06-17T00:45:13.919837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and metric\ndef jacard_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n\ndef jacard_coef_loss(y_true, y_pred):\n    return -jacard_coef(y_true, y_pred)  # -1 ultiplied as we want to minimize this value as loss function","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:18.051928Z","iopub.execute_input":"2022-06-17T00:45:18.052576Z","iopub.status.idle":"2022-06-17T00:45:18.099779Z","shell.execute_reply.started":"2022-06-17T00:45:18.052516Z","shell.execute_reply":"2022-06-17T00:45:18.098769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Extraction\n---------","metadata":{}},{"cell_type":"code","source":"# To read the CSV of annotations (labels)\nlabels_df = pd.read_csv(LABELS)\nlabels_df","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:55:47.027109Z","iopub.execute_input":"2022-06-16T03:55:47.02779Z","iopub.status.idle":"2022-06-16T03:55:47.301974Z","shell.execute_reply.started":"2022-06-16T03:55:47.027744Z","shell.execute_reply":"2022-06-16T03:55:47.301239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting the path of each image into a list\nfile_list = []\nimage_list = []\nn_cases = 156\nn_days = 39\nn_slices = 144\n\nfor case in range(n_cases+1):\n    for day in range(n_days+1):\n        for slices in range(n_slices+1):\n            if(slices<10):\n                str_slices = '000' + str(slices)\n            elif(slices<100):\n                str_slices = '00' + str(slices)\n            else:\n                str_slices = '0' + str(slices)\n                \n            for file in glob.glob(IMAGES + '/case' + str(case) + '/case' + str(case) + '_day' + str(day) +\n                                 '/scans/slice_' + str_slices + '*.png'):\n                \n                # Para enlazar a cada clase con una misma imagen\n                file_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:55:49.192579Z","iopub.execute_input":"2022-06-16T03:55:49.19337Z","iopub.status.idle":"2022-06-16T03:56:30.391905Z","shell.execute_reply.started":"2022-06-16T03:55:49.193333Z","shell.execute_reply":"2022-06-16T03:56:30.391033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list to match with their filenames\nn_classes = 3\nclass_column = []\nclasses = ['large_bowel', 'small_bowel', 'stomach']\nfor x in range(len(file_list)):\n    class_column.append(classes[0])\n    class_column.append(classes[1])\n    class_column.append(classes[2])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:30.393622Z","iopub.execute_input":"2022-06-16T03:56:30.394011Z","iopub.status.idle":"2022-06-16T03:56:30.413831Z","shell.execute_reply.started":"2022-06-16T03:56:30.393975Z","shell.execute_reply":"2022-06-16T03:56:30.413143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mask-Image Matching\n--------------","metadata":{}},{"cell_type":"code","source":"#  Matching a filename for each segmentation by using regular expressions\nfilename_column = []\ncase_column, day_column, slice_column = [], [], []\nheight_column, width_column, m_height_column, m_width_column = [], [], [], []\nid_column = []\n\nfor f in file_list:\n    \n    c = re.search('case(\\d+)_day', f)\n    d = re.search('_day(\\d+)/scans', f)\n    s = re.search('/slice_(\\d+)_', f)\n    i = 'case' + c.group(1) + '_day' + d.group(1) + '_slice_' + s.group(1)\n    height, width, m_height, m_width = get_metadata(f)\n    \n    for n in range(n_classes):\n        \n        filename_column.append(f)\n        \n        case_column.append(int(c.group(1)))\n        day_column.append(int(d.group(1)))\n        slice_column.append(int(s.group(1)))\n\n        height_column.append(int(height))\n        width_column.append(int(width))\n        m_height_column.append(float(m_height))\n        m_width_column.append(float(m_width))\n        \n        id_column.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:30.41521Z","iopub.execute_input":"2022-06-16T03:56:30.415563Z","iopub.status.idle":"2022-06-16T03:56:31.163764Z","shell.execute_reply.started":"2022-06-16T03:56:30.415527Z","shell.execute_reply":"2022-06-16T03:56:31.162918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Dataset Construction\n--------------","metadata":{}},{"cell_type":"code","source":"images_df = pd.DataFrame(list(zip(id_column, class_column, case_column, day_column, slice_column, filename_column,\n                                height_column, width_column, m_height_column, m_width_column)),\n                       columns = ['id', 'class', 'case', 'day', 'slice', 'filename', 'height', 'width',\n                                  'm_height', 'm_width'])\nimages_df","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:31.165859Z","iopub.execute_input":"2022-06-16T03:56:31.166253Z","iopub.status.idle":"2022-06-16T03:56:31.468397Z","shell.execute_reply.started":"2022-06-16T03:56:31.166216Z","shell.execute_reply":"2022-06-16T03:56:31.467621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"complete_data = pd.merge(labels_df, images_df, how='inner', on = ['id', 'class'])\ncomplete_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"complete_data.to_csv(r'./complete_data.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis\n---------","metadata":{}},{"cell_type":"code","source":"complete_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_df = complete_data.groupby([\"height\",'width'], as_index=False)['filename'].count()\nsize_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes_df = complete_data.groupby(\"class\", as_index=False)['segmentation'].count()\nclasses_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases_df = complete_data.groupby(\"case\", as_index=False)['filename'].count()\ncases_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases_df = complete_data.groupby(\"case\", as_index=False)['filename'].count()\ncases_df.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cases_graph = complete_data.groupby(\"case\").size()\nsns.barplot(x = cases_graph.index, y = cases_graph.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days_df = complete_data.groupby(\"day\", as_index=False)['filename'].count()\ndays_df # 91% of the images are of the day 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days_graph = complete_data.groupby([\"day\"]).size()\n \nsns.barplot(x = days_graph.index, y = days_graph.values/3)\nplt.savefig('day_description.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class Encoding\n--------------\n0. Unlabeled background\n1. Large bowel\n2. Small bowel\n3. Stomach","metadata":{}},{"cell_type":"code","source":"complete_data['class'] = complete_data['class'].replace('large_bowel', 1)\ncomplete_data['class'] = complete_data['class'].replace('small_bowel', 2)\ncomplete_data['class'] = complete_data['class'].replace('stomach', 3)\n\ncomplete_data","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:31.566932Z","iopub.execute_input":"2022-06-16T03:56:31.567314Z","iopub.status.idle":"2022-06-16T03:56:31.642106Z","shell.execute_reply.started":"2022-06-16T03:56:31.567278Z","shell.execute_reply":"2022-06-16T03:56:31.641437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segmentation decoding\n----------","metadata":{}},{"cell_type":"code","source":"complete_data['segmentation'] = complete_data['segmentation'].replace(np.nan, '0')     \ncomplete_data","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:57:11.561407Z","iopub.execute_input":"2022-06-16T03:57:11.561793Z","iopub.status.idle":"2022-06-16T03:57:12.702435Z","shell.execute_reply.started":"2022-06-16T03:57:11.561757Z","shell.execute_reply":"2022-06-16T03:57:12.701612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"complete_data[\"segmentation\"] = complete_data.apply(lambda x: rle2mask(mask_rle = x[\"segmentation\"],\n                                                                       label = 1,\n                                                                       shape = (x[\"height\"],x[\"width\"])), axis=1)\ncomplete_data","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:46.374451Z","iopub.execute_input":"2022-06-16T03:56:46.374862Z","iopub.status.idle":"2022-06-16T03:56:46.530001Z","shell.execute_reply.started":"2022-06-16T03:56:46.374824Z","shell.execute_reply":"2022-06-16T03:56:46.528709Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Column drop\n---------","metadata":{}},{"cell_type":"code","source":"complete_data = complete_data.drop(['id',\n                                    'case',\n                                    'day',\n                                    'slice',\n                                   'height',\n                                   'width',\n                                   'm_height',\n                                   'm_width'],\n                                   axis=1)\ncomplete_data","metadata":{"execution":{"iopub.status.busy":"2022-06-16T03:56:45.257074Z","iopub.execute_input":"2022-06-16T03:56:45.257612Z","iopub.status.idle":"2022-06-16T03:56:46.37204Z","shell.execute_reply.started":"2022-06-16T03:56:45.257575Z","shell.execute_reply":"2022-06-16T03:56:46.371115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mask Unification\n----------","metadata":{}},{"cell_type":"code","source":"os.mkdir('./dataset')\nos.mkdir('./dataset/images')\nos.mkdir('./dataset/masks')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creation of general masks and dataset\nfor x in range(0, len(complete_data), 3):\n    \n    image = cv2.imread(complete_data['filename'].iloc[x], -1)\n    image = image[(image.shape[0] - img_shape[0]):, (image.shape[1] - img_shape[1]):]\n    \n    mask = np.zeros(complete_data[\"segmentation\"].iloc[x].shape, dtype=np.int8)\n    \n    mask_lb = complete_data[\"segmentation\"].iloc[x]\n    \n    mask_sb = complete_data[\"segmentation\"].iloc[x + 1]\n    \n    mask_st = complete_data[\"segmentation\"].iloc[x + 2]\n   \n    mask[mask_lb == 1] = 1 # Large bowel\n    mask[mask_sb == 1] = 2 # Small bowel\n    mask[mask_st == 1] = 3 # Stomach\n    \n    mask = mask[(mask.shape[0] - img_shape[0]):, (mask.shape[1] - img_shape[1]):]\n    \n    cv2.imwrite('./dataset/images/image_' + str(x // 3) + '.png', image)\n    cv2.imwrite('./dataset/masks/mask_' + str(x // 3) + '.png', mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive('data', 'zip', './dataset/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='./data.zip'> Download Dataset </a>","metadata":{}},{"cell_type":"markdown","source":"# Dataset Splitting\n----------","metadata":{}},{"cell_type":"code","source":"patoolib.extract_archive('./data.zip', outdir='./')\n\ninput_folder = './data'\n\nsplitfolders.ratio(input_folder, output=\"dataset\", seed=1337, ratio=(.8, .2), group_prefix=None) # train and valid\n\nshutil.make_archive('dataset', 'zip', './dataset')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization\n------------","metadata":{}},{"cell_type":"code","source":"n = 28110\nimg = cv2.imread(complete_data['filename'].iloc[n], -1)\nplt.imshow(img, cmap='gray')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask1 = rle2mask(complete_data['segmentation'].iloc[n],\n                1,\n                (complete_data['height'].iloc[n],complete_data['width'].iloc[n]))\nplt.imshow(mask1, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask2 = rle2mask(complete_data['segmentation'].iloc[n+1],\n                1,\n                (complete_data['height'].iloc[n+1],complete_data['width'].iloc[n+1]))\nplt.imshow(mask2, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask3 = rle2mask(complete_data['segmentation'].iloc[n+2],\n                1,\n                (complete_data['height'].iloc[n+2],complete_data['width'].iloc[n+2]))\nplt.imshow(mask3, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8))\nplt.subplot(2, 3, 2); plt.imshow(img, cmap='bone')  ;  plt.title('Image')\nplt.subplot(2, 3, 4); plt.imshow(mask1, cmap='bone');  plt.title('Large bowel')\nplt.subplot(2, 3, 5); plt.imshow(mask2, cmap='bone');  plt.title('Small bowel')\nplt.subplot(2, 3, 6); plt.imshow(mask3, cmap='bone');  plt.title('Stomach')\nfig.savefig('classes.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.zeros(img.shape, dtype=np.int8)\n    \nmask[mask1 == 1] = 1 # Large bowel\nmask[mask2 == 1] = 2 # Small bowel\nmask[mask3 == 1] = 3 # Stomach","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 7))\nplt.subplot(1, 3, 1); plt.imshow(img, cmap='bone');\nplt.axis('OFF'); plt.title('image')\nplt.subplot(1, 3, 2); plt.imshow(mask*255, cmap='hot'); plt.axis('OFF'); plt.title('mask')\nplt.subplot(1, 3, 3); plt.imshow(img, cmap='gray'); plt.imshow(mask*255, alpha=0.4);\nplt.axis('OFF'); plt.title('overlay')\nplt.tight_layout()\nplt.savefig('segmentation.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preproccesing and Dataset Creation\n-------------","metadata":{}},{"cell_type":"code","source":"img_shape = (256, 256, 1) # Required shape of the images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = CreateSet(TRAIN_IMAGES_PATH, TRAIN_MASKS_PATH, n_classes = 4, seed_sample = 15000)\nX_valid, Y_valid = CreateSet(VALID_IMAGES_PATH, VALID_MASKS_PATH, n_classes = 4, seed_sample = 3000)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:41:17.992239Z","iopub.execute_input":"2022-06-17T00:41:17.993056Z","iopub.status.idle":"2022-06-17T00:43:36.24351Z","shell.execute_reply.started":"2022-06-17T00:41:17.993006Z","shell.execute_reply":"2022-06-17T00:43:36.24244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' Train Dataset shape:     X_train --> {X_train.shape}    Y_train --> {Y_train.shape} \\n')\nprint(f' Valid Dataset shape:     X_valid --> {X_valid.shape}     Y_valid --> {Y_valid.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:43:36.245543Z","iopub.execute_input":"2022-06-17T00:43:36.246186Z","iopub.status.idle":"2022-06-17T00:43:36.252902Z","shell.execute_reply.started":"2022-06-17T00:43:36.246143Z","shell.execute_reply":"2022-06-17T00:43:36.252008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = 27\nplt.imshow(X_train[num], cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:13.414529Z","iopub.execute_input":"2022-06-16T13:59:13.415282Z","iopub.status.idle":"2022-06-16T13:59:13.934332Z","shell.execute_reply.started":"2022-06-16T13:59:13.415243Z","shell.execute_reply":"2022-06-16T13:59:13.933225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_mask = os.listdir(TRAIN_MASKS_PATH)\nlist_mask.sort()\nmsk = cv2.imread(TRAIN_MASKS_PATH + list_mask[15027],-1)\nplt.imshow(msk, cmap='hot')\nnp.unique(msk)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:13.936102Z","iopub.execute_input":"2022-06-16T13:59:13.936598Z","iopub.status.idle":"2022-06-16T13:59:14.134865Z","shell.execute_reply.started":"2022-06-16T13:59:13.936559Z","shell.execute_reply":"2022-06-16T13:59:14.13408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(Y_train[num,:,:,1], cmap='hot')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:24.481702Z","iopub.execute_input":"2022-06-16T13:59:24.482108Z","iopub.status.idle":"2022-06-16T13:59:24.507412Z","shell.execute_reply.started":"2022-06-16T13:59:24.482077Z","shell.execute_reply":"2022-06-16T13:59:24.506418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train[num].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Avoiding Overfitting\n-----------","metadata":{}},{"cell_type":"code","source":"seed=24\nbatch_size= 16\n\ntrain_img_gen = Generator(X_train, Y_train, n_classes = 4)\n\nval_img_gen = Generator(X_valid, Y_valid, n_classes = 4)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:44:58.616058Z","iopub.execute_input":"2022-06-17T00:44:58.617157Z","iopub.status.idle":"2022-06-17T00:44:58.622667Z","shell.execute_reply.started":"2022-06-17T00:44:58.617105Z","shell.execute_reply":"2022-06-17T00:44:58.621617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Another option to set weights to the classes\nclass_weights_manual = {0: 0.01,\n                1: 3.0,\n                2: 3.5,\n                3: 4.5}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Measures\n------------","metadata":{}},{"cell_type":"markdown","source":"## Configuration 1 (Accuracy)","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.compile(optimizer= Adam(learning_rate = 1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T20:58:48.799058Z","iopub.execute_input":"2022-06-16T20:58:48.799681Z","iopub.status.idle":"2022-06-16T20:58:49.166219Z","shell.execute_reply.started":"2022-06-16T20:58:48.799631Z","shell.execute_reply":"2022-06-16T20:58:49.165217Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration 2 (IoU Loss)","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.compile(optimizer= Adam(learning_rate = 1e-4), loss='categorical_crossentropy', metrics=[jacard_coef])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:21.251546Z","iopub.execute_input":"2022-06-17T00:45:21.251972Z","iopub.status.idle":"2022-06-17T00:45:24.091546Z","shell.execute_reply.started":"2022-06-17T00:45:21.251935Z","shell.execute_reply":"2022-06-17T00:45:24.090523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n----------","metadata":{}},{"cell_type":"code","source":"n_classes = 4\nIMG_HEIGHT = 256\nIMG_WIDTH  = 256\nIMG_CHANNELS = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:12.269993Z","iopub.execute_input":"2022-06-17T00:45:12.270395Z","iopub.status.idle":"2022-06-17T00:45:12.274864Z","shell.execute_reply.started":"2022-06-17T00:45:12.270364Z","shell.execute_reply":"2022-06-17T00:45:12.274047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [ModelCheckpoint('./unet.h5', verbose=1, save_best_only=True, save_weights_only=True, monitor='val_loss'),\n            EarlyStopping(monitor=\"val_loss\", patience=15, verbose=2, mode=\"auto\", restore_best_weights=True)]","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:45:29.941482Z","iopub.execute_input":"2022-06-17T00:45:29.941924Z","iopub.status.idle":"2022-06-17T00:45:29.949176Z","shell.execute_reply.started":"2022-06-17T00:45:29.941887Z","shell.execute_reply":"2022-06-17T00:45:29.946818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_imgs = X_train.shape[0]\nsteps_per_epoch = num_train_imgs // batch_size","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_img_gen, \n                    #batch_size = 16,\n                    verbose=1, \n                    epochs=200,\n                    validation_data=val_img_gen,\n                    steps_per_epoch=steps_per_epoch, \n                    validation_steps=steps_per_epoch,\n                    callbacks = callbacks)\n                    #class_weight=class_weights_manual) #Is not supported for 3D\n                    #shuffle=False)\n                    \nmodel.save('second_train_200epochs_batchsize16_lr4_dataaug_weights30percentimages.hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:46:02.017619Z","iopub.execute_input":"2022-06-17T00:46:02.018404Z","iopub.status.idle":"2022-06-17T02:19:08.081338Z","shell.execute_reply.started":"2022-06-17T00:46:02.018345Z","shell.execute_reply":"2022-06-17T02:19:08.079277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training and validation loss\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nfig = plt.figure()\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('30percent_Train_val_loss_200epochs_batchsize8_lr4_dataaug.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:19:08.086218Z","iopub.execute_input":"2022-06-17T02:19:08.086522Z","iopub.status.idle":"2022-06-17T02:19:08.416525Z","shell.execute_reply.started":"2022-06-17T02:19:08.086495Z","shell.execute_reply":"2022-06-17T02:19:08.415718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training and validation metric\nplt.plot(history.history['jacard_coef'])\nplt.plot(history.history['val_jacard_coef'])\nplt.title('model jacard_coef')\nplt.ylabel('jacard_coef')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.savefig('PREDICEJACCARDCOEFTrain_val_loss_200epochs_batchsize8_lr4_dataaug.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:24:09.649783Z","iopub.execute_input":"2022-06-17T02:24:09.650879Z","iopub.status.idle":"2022-06-17T02:24:09.927569Z","shell.execute_reply.started":"2022-06-17T02:24:09.650805Z","shell.execute_reply":"2022-06-17T02:24:09.926667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for metric\nplt.figure(figsize=(16, 8))\nplt.subplot(231)\nplt.plot(history.history['jacard_coef'])\nplt.plot(history.history['val_jacard_coef'])\nplt.title('model jacard_coef')\nplt.ylabel('jacard_coef')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(232)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.savefig('30percent_Train_val_loss_200epochs_batchsize16_lr4_dataaug.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:19:08.418027Z","iopub.execute_input":"2022-06-17T02:19:08.418414Z","iopub.status.idle":"2022-06-17T02:19:08.860405Z","shell.execute_reply.started":"2022-06-17T02:19:08.418373Z","shell.execute_reply":"2022-06-17T02:19:08.859579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n-------","metadata":{}},{"cell_type":"code","source":"n = 30000 #77\nlist_images = os.listdir(TRAIN_IMAGES_PATH)\nlist_images.sort()\nimg_prueba = cv2.imread(TRAIN_IMAGES_PATH + list_images[n], -1)\nplt.imshow(img_prueba, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:29:19.386009Z","iopub.execute_input":"2022-06-17T02:29:19.386685Z","iopub.status.idle":"2022-06-17T02:29:19.694751Z","shell.execute_reply.started":"2022-06-17T02:29:19.386631Z","shell.execute_reply":"2022-06-17T02:29:19.693877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_masks = os.listdir(TRAIN_MASKS_PATH)\nlist_masks.sort()\nmask_prueba = cv2.imread(TRAIN_MASKS_PATH + list_masks[n], -1)\nplt.imshow(mask_prueba, cmap='hot')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:29:21.935934Z","iopub.execute_input":"2022-06-17T02:29:21.936561Z","iopub.status.idle":"2022-06-17T02:29:22.149349Z","shell.execute_reply.started":"2022-06-17T02:29:21.936524Z","shell.execute_reply":"2022-06-17T02:29:22.148521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"P = []\nimage = Image.fromarray(img_prueba)\nP.append(np.array(image))\nP = np.array(P)\nP = P[...,None] # Expand dims\nP = P / (2 ** 16 - 1.) # Normalization is performed by 2^16 because the image is uint16","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:29:27.308397Z","iopub.execute_input":"2022-06-17T02:29:27.308939Z","iopub.status.idle":"2022-06-17T02:29:27.318216Z","shell.execute_reply.started":"2022-06-17T02:29:27.308891Z","shell.execute_reply":"2022-06-17T02:29:27.316178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = model.predict(P)\nmask_predict = predict[0].argmax(-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:29:31.219418Z","iopub.execute_input":"2022-06-17T02:29:31.22009Z","iopub.status.idle":"2022-06-17T02:29:31.271769Z","shell.execute_reply.started":"2022-06-17T02:29:31.220041Z","shell.execute_reply":"2022-06-17T02:29:31.270884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10,10))\nax1.imshow(img_prueba, cmap='gray')\nax2.imshow(mask_prueba, cmap='hot')\nax3.imshow(mask_predict, cmap='hot')\nfig.savefig('Prediction.png')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T02:33:14.373122Z","iopub.execute_input":"2022-06-17T02:33:14.374043Z","iopub.status.idle":"2022-06-17T02:33:14.815051Z","shell.execute_reply.started":"2022-06-17T02:33:14.374001Z","shell.execute_reply":"2022-06-17T02:33:14.814209Z"},"trusted":true},"execution_count":null,"outputs":[]}]}